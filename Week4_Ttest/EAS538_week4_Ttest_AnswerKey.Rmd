---
title: "EAS 538 T-test (Answer Key)"
author: "Written by Oscar Feng-Hsun Chang and modified by Arthur Endsley"
date: "Week 4"
output: 
  html_document:
    code_folding: show
    highlight: textmate
    keep_md: no
    number_sections: false
    theme: flatly
    toc: yes
    toc_float:
      collapsed: false
      smooth_scroll: true
      toc_depth: 4
---

```{r setup, echo=FALSE}
apdata <- read.csv('https://raw.githubusercontent.com/OscarFHC/EAS538_2018/master/Week3_CLT/aps.csv')

meansVector <- function(data, times, size, var) {
  v <- c()
  for (i in 1:times) {
    y <- sample(data[,var], size, replace=TRUE)
    m <- mean(y)
    v[i] <- m
  }
  return(v)
}

cifunz <- function(means, zcrit, sem) {
  cilower <- means - zcrit*sem
  ciupper <- means + zcrit*sem
  civals <- c(cilower, ciupper)
  return(civals)
}

cifunt <- function(means, tcrit, sem) {
  cilower <- means - tcrit*sem
  ciupper <- means + tcrit*sem
  civals <- c(cilower, ciupper)
  return(civals)
}
```

# Exercise 1

## Part 1

> Write out the null and alternative hypothesis for a one-way, two-tailed t-test using the example above. What did your results tell you? Can you reject the null hypothesis (at the 0.05 level)?  

__Null Hypothesis:__ The mean concentration of pesticide residue on each apple tested is not different from the USDA limit, 0.1477.  

__Alternative Hypothesis:__ The mean concentration of pesticide residue on each apple tested is different (either greater or less) from the USDA limit, 0.1477.

```{r, one sample, two tailed t-test,eval=FALSE}
t.test(apdata[,"concen"], mu = 0.1477)
```

From the output, there are two ways we can judge the null hypothesis.  

1. The 95% confidence interval of the sample mean is from `r t.test(apdata[,"concen"], mu = 0.1477)$ conf.int[1]` to `r t.test(apdata[,"concen"], mu = 0.1477)$ conf.int[2]`. This does covers the reference value (0.1477), so we can't reject the null hypothesis given the data we have. Or, another way to put it is that we don't have enough data(evidence) to reject the null hypothesis.  
2. The p-value of this two-tail test is `r t.test(apdata[,"concen"], mu = 0.1477)$p.value`, which is greater than the $\alpha$ value we set (0.05). We can't reject the null hypothesis given the data we have.

*The third way to judge the null hypothesis is to compare the t-statistics of the reference value (0.1477) based on the data to the critical t-values corresponding to the $\alpha$ value we set (0.05). However the critical t-values are not reported in the output*

## Part 2

> Write out the null and alternative hypothesis for a one-way, one-tailed t-test using the example above. What did your results tell you? Can you reject the null hypothesis (at the 0.05 level)?

__Null Hypothesis 1:__ The mean concentration of pesticide residue on each apple tested is not less than the USDA limit, 0.1477.  

__Alternative Hypothesis 1:__ The mean concentration of pesticide residue on each apple tested is less than from the USDA limit, 0.1477.

**NOTE:** For the null hypothesis, you could write either "is not less than" or "is greater than or equal to." Remember that the null hypothesis is what you're trying to disprove. It would be a problem for our apple company if the resticide concentration was either "not less than" or "greater than or equal to" the USDA limit.

```{r, one sample, two tailed t-test, H01}
t.test(apdata[,"concen"], alternative = "less", mu = 0.1477)
```

Similarly, from the output, we there are two ways we can judge the null hypothesis. 

1. The 95% confidence interval of the sample mean is from `r t.test(apdata[,"concen"], alternative = "less", mu = 0.1477)$ conf.int[1]` to `r t.test(apdata[,"concen"], alternative = "less", mu = 0.1477)$ conf.int[2]`. This does NOT covers the reference value (0.1477), so we can reject the null hypothesis given the data we have. Or, we can say, we have enough data (evidence) to reject the null hypothesis.  
2. The p-value of this two-tail test is `r t.test(apdata[,"concen"], alternative = "less", mu = 0.1477)$p.value`, which is less than the $\alpha$ value we set (0.05). We can reject the null hypothesis given the data we have.

*Again, the critical t-values are not reported in the output, so we can't judge the null hypothesis by comparing the t-statistics of the reference value (0.1477) based on the data to the critical t-values corresponding to the $\alpha$ value we set (0.05).*

# Exercise 2

## Part 1

> Why did you use 0.95 in your qt function instead of 0.975 (which is what you did last week)?

```{r, parameterize and run CI}
meanval <- mean(apdata$concen)
tcritval <- qt(0.95, df = length(apdata$concen) - 1)
semval <- (sd(apdata$concen) / sqrt(length(apdata$concen)))

cifunt(meanval, tcritval, semval)
```

The `cifunt` (and also `cifunz`) is a general-purpose function we wrote for confidence intervals, so it will always output both lower and upper bound of a specified confidence level according to the value input into `qt()` (or `qnorm()`). Take this case for example, because we put *0.95* in the `qt()` function, `cifunt()` function will output both the lower and upper bound of *90%* confidence level. However, since we want to use this function (`cifunt()`) for an one-tail test, we compare the reference value (0.1477) to one of these two boundaries. By doing so, the confidence level becomes `-Inf` to the upper bound (or the lower bound to `Inf`). This confidence interval covers 95% of the data (because we include one side of the 5%) and thus becomes a 95% confidence interval. 

Put another way... Before, for a 95% confidence interval, we wanted the critical values in each tail: at 2.5% (0.025) and 97.5% (0.975). Now, we have a one-tailed test and the 5% false-positive risk is shifted to just one of the tails (taking 2.5% from one tail and adding it to the other makes 5%). The t-distribution is symmetric so we could use either the critical value at 5% (0.05) or at 95% (0.95) as long as we're careful to pay attention to the sign. The critical value at 5% will be negative and the critical value at 95% will be positive, but they have the same absolute value. The absolute value of our t-statistic must be greater than the absolute value of our critical t-value in order to declare a significant result.

## Part 2
  
> Can you reject the null hypothesis based on the confidence intervals that you calculated?

If we are testing the following null and alternative hypothesis. 

__Null Hypothesis 1:__ The mean concentration of pesticide residue on each apple tested is greater than or equal to the USDA limit, 0.1477.  

__Alternative Hypothesis 1:__ The mean concentration of pesticide residue on each apple tested is less than from the USDA limit, 0.1477.

We use only the upper boundary for this null hypothesis, so that the confidence interval is from -Inf to `r cifunt(meanval, tcritval, semval)[2]`. The reference value (0.1477) falls out side of the confidence interval, so we can reject the null hypothesis. 

Similarly, we can calculate the confidence interval base on another set of null and alternative hypothesis. 

__Null Hypothesis 2:__ The mean concentration of pesticide residue on each apple tested is greater than or equal to the USDA limit, 0.1477.  

__Alternative Hypothesis 2:__ The mean concentration of pesticide residue on each apple tested is less than from the USDA limit, 0.1477.

In this case, the confidence interval becomes `r cifunt(meanval, tcritval, semval)[1]` to Inf. So that we can't reject this null hypothesis. 

In conclusion, by calculating the confidence interval, we have the same results and the confidence interval we manually calculated matched those from the `t.test()` function. This is the second way of judging a null hypothesis. 

## Part 3

> How would you change the code above if you ran a two-tailed, one-way t-test? Please calculate 95% confidence intervals for a two-tailed, one-way t-test and tell us whether you can reject the null hypothesis. 

```{r}
meanval <- mean(apdata$concen)
tcritval <- qt(0.975, df = length(apdata$concen) - 1)
semval <- (sd(apdata$concen) / sqrt(length(apdata$concen)))

cifunt(meanval, tcritval, semval)
```

Because we are doing a two-tail test, we should use both lower and upper boundaries. In this case, we should modify the input for `qt()` function from 0.95 to 0.975. **NOTE:** This is basically the reverse of what we explained in Part 1: we shifted 2.5% out of one tail and made a total of 5% in the other. Now, we're taking 2.5% out of the one tail and putting it in the other. The confidence level (95%) and the total false positive risk (5%) are the same in both cases.

In this case, the null and alternative hypothesis become as follows:

__Null Hypothesis:__ The mean concentration of pesticide residue on each apple tested is not different from the USDA limit, 0.1477.  

__Alternative Hypothesis:__ The mean concentration of pesticide residue on each apple tested is different from the USDA limit, 0.1477.

The reference value (0.1477) falls within the confidence level, so that we cannot reject the null hypothesis give the data. 

## Part 4

> Let's use the `cifunz` function written above and `qnorm` to calculate 95% confidence intervals of a one-tailed, one-way test using the normal distribution. Can you reject the null hypothesis? How do the confidence intervals you calculated here (using the standard normal distribution) compare to those calculated originally (using the *t* distribution)?

This question is a bit tricky. We intend to have you calculate the condifence interval with the `cifunz()` based on the data, not the resampled means (the `mean_vector` variable). Ideally, you will find that the confidence interval calculated based on t-distribution and z-distribution (standard normal distribution) are similar because the sample size is sufficiently large. With over 15,000 degrees of freedom, the t-distribution is virtually identical to the standard normal distribution.

```{r}
meanval <- mean(apdata$concen)
zcritval <- qnorm(0.975)
semval <- (sd(apdata$concen) / sqrt(length(apdata$concen)))

cifunz(meanval, zcritval, semval)
```

However, because the data are *NOT* normally distributed, technically, we can't use t-test on this data. Although the mean of the data is still from a normal distribution, the standard deviation of this normal distribution should not be estimated by the standard error of the sample mean. 

```{r, mean function run}
mean_vector <- meansVector(apdata, 10000, 1000, 'concen')
```

We can see this by comparing the standard error of the sample mean (`r semval`) to the standard deviation of the distribution of the means (the standard deviation of the `mean_vector` variable; `r sd(mean_vector)`) Although we have a large enough sample size, the t-test is still the best practice. Provided our sample size isn't too small, **we shouldn't be overly concerned** if our data appear to violate the normality assumption. 

Moreover, I include a simulation in the end of this document demonstrating you that violating the normal distribution assumption should not affect the results too much. [An article on the Stats StackExchange](https://stats.stackexchange.com/questions/9573/t-test-for-non-normal-when-n50) also contains similar assertions about robustness to non-normal data:  

* "The t-test assumes that the means of the different samples are normally distributed; it does not assume that the population is normally distributed."
* "For approximately normal distributions, you won't need as large sample as a very non-normal distribution."
* "As far as I know, t-test is fairly resistent to moderate deviations from normality."

# Exercise 3

> How would you use the above results to calculate the t-statistic, df, and p-value of a two-tailed, one-way t-test?  

```{r, echo=FALSE}
tstat <- function(samplem, refval, sem) {
  val <- (samplem - refval)/sem
  return(val)
}

samplem <- mean(apdata$concen)
refval <- 0.1477
sem <- sd(apdata$concen) / sqrt(length(apdata$concen))
tval <- tstat(samplem, refval, sem)

pval <- pt(tval, df = length(apdata$concen) - 1)

pval_two <- 2 * pval
```

```{r}
x <- seq(from = -4, to = 4, 0.1)
plot(x, dt(x, df = length(apdata$concen) - 1), xlab = '', ylab = 'Frequency', main = 'Standardized t distribution (df=15775)', type = 'l') # this just plots the t distribution for the df of the apdata set
abline(v = tval, col = 'red')
abline(v = -tval, col = 'red')
cord.x1 <- c(-4, seq(from = -4, to = tval, 0.1), tval) 
cord.y1 <- c(0, dt(seq(from = -4,to = tval, 0.1), df = length(apdata$concen) - 1), 0) 
polygon(cord.x1, cord.y1, col = "grey30", border = NA)
cord.x2 <- c(-tval, seq(from = -tval, to = 4 , 0.1), 4) 
cord.y2 <- c(0, dt(seq(from = -tval,to = 4, 0.1), df = length(apdata$concen) - 1), 0) 
polygon(cord.x2, cord.y2, col = "grey30", border = NA)
text(0, 0.1, 'grey area = 9.2 %')
text(-2.4, 0.2, 't-statistic \n = -1.683735')
text(2.3, 0.2, 't-statistic \n = 1.683735')
```

The degrees of freedom are the same (15775) because our sample size hasn't changed. The t-statistic is based only on our sample mean and the degrees of freedom, so it hasn't changed either. The p-value for this proposed two-tail test can then be found as simply twice the p-value of the one-tailed test.

In detail: From the above calculation, the t-statistic is `r tval`, which is based on the mean (`r samplem`) and the degree of freedom (`r length(apdata$concen) - 1`) of the data. The p-value (`r pval`) is the probability of seeing any t-statistic that is less than the t-statistic of the reference value (`r tval`). However, if it is a two-tail test, we should consider the probability of of seeing any t-statistic that is less than the t-statistic of the reference value (`r tval`) and any t-statistic that is greater than the opposite the t-statistic of the reference value (`r -tval`). 

# Exercise 4

## Part 1

> Please write out the null and alternate hypothesis for the t-test above. Are you able to reject the null hypothesis? What does your result mean in non technical terms?    

```{r}
iris_sub <- subset(iris, Species %in% c('setosa', 'versicolor'))
t.test(Sepal.Length ~ Species, data = iris_sub)
```

__Null Hypothesis:__ The mean Sepal length of _setosa_ species is not different from the mean Sepal length of _versicolor_. 

__Alternative Hypothesis:__ The mean Sepal length of _setosa_ species is different from the mean Sepal length of _versicolor_. 

Based on the condifernce interval or the p-value, we can reject the null hypothesis. This means the mean Sepal length of _setosa_ species is different from the mean Sepal length of _versicolor_ given the data we have. 

## Part 2

> Please repeat the above analysis for `Sepal.Width`, `Petal.Length`, and `Petal.Width` for the `iris_sub` dataset. Please interpret the results of these 3 t-tests in non-technical terms. 

### Comparing the Sepal width

```{r}
iris_sub <- subset(iris, Species %in% c('setosa', 'versicolor'))
t.test(Sepal.Width ~ Species, data = iris_sub)
```

__Null Hypothesis:__ The mean Sepal width of _setosa_ species is not different from the mean Sepal width of _versicolor_. 

__Alternative Hypothesis:__ The mean Sepal width of _setosa_ species is different from the mean Sepal width of _versicolor_. 

Based on the condifedence interval or the p-value, we can reject the null hypothesis. This means the mean Sepal width of _setosa_ species is different from the mean Sepal width of _versicolor_ given the data we have. 

### Comparing the Petal length 

```{r}
iris_sub <- subset(iris, Species %in% c('setosa', 'versicolor'))
t.test(Petal.Length ~ Species, data = iris_sub)
```

__Null Hypothesis:__ The mean Petal length of _setosa_ species is not different from the mean Petal length of _versicolor_. 

__Alternative Hypothesis:__ The mean Petal length of _setosa_ species is different from the mean Petal length of _versicolor_. 

Based on the condifernce interval or the p-value, we can reject the null hypothesis. This means the mean Petal length of _setosa_ species is different from the mean Petal length of _versicolor_ given the data we have. 

### Comparing the Petal Width 

```{r}
iris_sub <- subset(iris, Species %in% c('setosa', 'versicolor'))
t.test(Petal.Width ~ Species, data = iris_sub)
```

__Null Hypothesis:__ The mean Petal width of _setosa_ species is not different from the mean Petal width of _versicolor_. 

__Alternative Hypothesis:__ The mean Petal width of _setosa_ species is different from the mean Petal width of _versicolor_. 

Based on the condifernce interval or the p-value, we can reject the null hypothesis. This means the mean Petal width of _setosa_ species is different from the mean Petal width of _versicolor_ given the data we have.

# Consequence of violating normal distribution assumption

Let's violate the second assumption, normal distribution, with the same method and see what would happen...  

Let's compare a normal distribution to a [gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution), which is normally a right-skewed distribution. Can you plot the following histogram showing the gamma distribution?

```{r, echo=FALSE}
gamma <- rgamma(10000, shape=5, scale=1/5)
hist(gamma, breaks=30, probability=TRUE, main="probability density function of gamma distribution (shape=5; scale=1/5)")
lines(density(gamma), col="red")
```

Here, I set the mean of gamma distribution to be the same with the normal distribution, so that the null hypothesis should still always be true. In this case, I compare 10 numbers from normal distribution to 10 numbers from gamma distribution. The sample size is fairly small. **NOTE:** If you run this next code block it may take a long time. Monte Carlo simulations usually do!

```{r}
t_stat <- function(x, y) { # generates values of t-test statistic
  
  n <- length(x)
  m <- length(y)
  
  # compute pooled standard error   
  sp <- sqrt(((n-1)*sd(x)^2 + (m-1)*sd(y)^2)/(n+m-2)*(n+m)/(n*m))
  
  # compute test statistic
  (mean(x)-mean(y))/sp
}

n <- 10
m <- 10
crit <- c(qt(0.025, n+m-2), qt(0.975, n+m-2))

nsim <- 1000 # number of repetitions for ONE Monte Carlo simulation,
            # produces ONE estimate of the probability of a type 1 error.
nrep <- 1000 # number of repetitions of Monte Carlo simulation of length nsim,
           # needed to compute several (nrep) estimates of the probability of type 1 error

val <- matrix(NA, ncol=1, nrow=nsim)
prob <- matrix(NA, ncol=1, nrow=nrep)

# Monte Carlo Simulation
set.seed(1032)

for (i in 1:nrep) {
  for (j in 1:nsim) {
    
    x <- rnorm(n, 1, 1)
    y <- rgamma(m, shape=5, scale=1/5)

    # values of t-test statistic
    val[j] <- t_stat(x,y)
  }
  
  # probability of type 1 error
  prob[i] <- length(val[val> crit[2] | val< crit[1]])/ length(val)
}
p <- round(mean(prob),3)
se <- round(sd(prob)/sqrt(nrep),7)

print(paste0("probability of making type I error = ", p));
print(paste0("Standard error of the probability of making type I error = ", se))
```

We see that the probability of making type I error becomes `r p`. It also increase the probability of making Type I error. However, the probability of committing type-I error only increased by 0.006, although we set the $\alpha$ value to 0.05. From this simulation, we see that violating normality assumption *WILL* increase the probability of committing type-I error, but not so severe. Recall that the sample size is pretty small (10 VS 10). When increasing sample size to 30 VS 30, the the probability of committing type-I error only increased by about 0.002, which should further alleviate your anxiety from violating normality assumption.  
Again, we should *NOT* use t-test when the data is not normally distributed if we have better option. If not, we don't have to be too worry as t-test is fairly robust. 
