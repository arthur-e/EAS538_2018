---
title: "Take-Home Test"
author: "Andrew Kinzer"
date: "April 20, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(lmtest)
library(dunn.test)
library(car)
library(interplot)
library(gmodels)
library(relimp)
library(relaimpo)
library(caret)

flying=read.table(file="https://raw.githubusercontent.com/OscarFHC/NRE538_2017Fall/master/Final/flying.csv",header=TRUE, sep=",")

college = read.table(file="https://raw.githubusercontent.com/OscarFHC/NRE538_2017Fall/master/Final/college.csv",header=TRUE, sep=",")

happy = read.table(file="https://raw.githubusercontent.com/OscarFHC/NRE538_2017Fall/master/Final/happy.csv",header=TRUE, sep=",")

cancer = read.table(file="https://raw.githubusercontent.com/OscarFHC/NRE538_2017Fall/master/Final/cancer.csv",header=TRUE, sep=",")
```

###Question 1

Is there a significant association between gender (gender) and whether people think it's rude to bring an unruly child on the plane (unruly_child)? If yes, which gender tends to think that bringing an unruly child is more rude?

First we need to make a table summarizing our data.
```{r}
cont.table=table(flying$gender, flying$unruly_child)
cont.table
```

Then run a chi-square test using the table we created.
```{r}
chisq.test(cont.table)

```

With a p-value=0.001193, it would appear that males and females view taking unruly children on flights differently.
From the table, we can calculate that 79% of females think that bringing unruly children on flights is somewhat to very rude, while 86% of men think that bringin unruly children on flights is somewhat to very rude. It seems  men think bringing unruly children on flights is more rude than women.


###Question 2
Is there a significant difference in tuition by type of institution? If yes, which type has a highter tuition?

```{r echo=TRUE}
boxplot(tuition~type, data=college, main="Private vs Public College Tuition Cost")
```

It seems like there could be a difference in private non-profit tuition and public costs. 

```{r echo=TRUE}
hist(college$tuition, main= "Histogram of College Tuition", xlab="College Tuition")
```

The data looks slightly skewed, but because we have such a large sample size (1407), we can assume that the data is normally distributed.

I also checked for equal variances

```{r echo=TRUE}
tuition.private=subset(college, type=="Private nonprofit")
tuition.public=subset(college, type=="Public")
var.test(tuition.private[,"tuition"], tuition.public[,"tuition"])
```

With a very small p-value, we reject the null that our variances are equal. Because the variances aren't equal, but our data is large enough to assume normality I will use a Welche's t-test to see if there is a difference in tuition based on whether a college is public or private.


```{r echo=TRUE}
t.test(tuition.private$tuition, tuition.public$tuition)
```

Based on the Welche's two sample t-test, we can reject the null hypothesis that there is no difference in tuition between the two types of colleges. From the boxplot, it appears that the Private nonprofit Colleges charge a statistically significantly higher tuition rate than Public colleges.

###Question 3

Is there a significant difference in happiness by region?

Because we have a continuous dependent variable, and an indpendent, categorical variable, we'll run a one-way ANOVA to check if there is a difference in people's happiness by region. 

```{r}
boxplot(Hscore~Region, data=happy, main="Happiness Score by Region of the World")
```

The boxplot would indicate that there is likely a difference in Hscore based on region.


```{r echo=TRUE}
hist(happy$Hscore, main="Histogram of Happiness Score", xlab="Happiness Score")
```

When plotting the dependent variable, it appears that it is fairly normal. It also has a large sample size, so we should be fine.

We should check that the populations have the same variance. We can use the Levene Test to do this.

```{r echo=TRUE}
leveneTest(Hscore~Region, data=happy)
```

We cannot reject the null that our variances are homogenous, so we pass that assumption. 


```{r echo=TRUE}
happyaov=aov(Hscore~Region, data=happy)
summary(happyaov)
```

The result of our ANOVA tells us that the happiness score does vary by Region, however it does not tell us which how it varies.  

###Question 4 

What factors are significantly associated with a country's corruption levels? Choose three continuous independent variables to include in the model. 

```{r echo=TRUE}
hist(happy$Corruption, main="Histogram of Corruption Levels", xlab="Corruption")
```

The corruption data does not look normally distributed. So I'll try a log a square root transform to see if I can get it to look more normal. I will pick the transform that gets my data closer to being normal distributed, and that will also still pass the other assumptions necessary to run a Linear Model.

```{r echo=TRUE}
happyT=mutate(happy, logcorupt=log(Corruption))
happyT=mutate(happyT, sqrtcor=sqrt(Corruption))
hist(happyT$logcorupt, main="Histogram of Log Transformed Corruption", xlab="Log Corruption")
hist(happyT$sqrtcor, main="Histogram of Square Root Transformed Corruption", xlab="Sqrt Corruption")
```

I like the square root transform better than the log, so I'll go with that transformation moving forward. I will, however, run both the square root transform and the log transformed variables through the linear model just to check how the normality and the homoscedasticity compare. 
```{r}
pairs(happyT)
```

This gives me a starting point to make sure that when I pick variables to include in my model, I can avoid picking variables that are highly correlated with each other. Looking at this chart, it looks like Freedom, GDP, and Generosity could be decent continous candidates. We can check their correlation values, though, to make sure we're not picking highly correlated variables. 

```{r echo=TRUE}
cor(happyT[, c("Freedom","GDP","Generosity")])
```

I'm pretty satisfied that these variables aren't that highly correlated with each other, so I'll go ahead and include them in my model. 


```{r}
mod0=lm(Corruption~Freedom+GDP+Generosity, data=happyT)
summary(mod0)
plot(mod0)
```

This qqplot just confirms the fact that my untransformed corruption variable was not normally distributed....The horn-shaped plot of the residuals also indicates that the data is also not really homoscedastic either. 

```{r}
mod1=lm(sqrtcor~Freedom+GDP+Generosity, data=happyT)
plot(mod1)
```

 

```{r echo=TRUE}
mod2=lm(logcorupt~Freedom+GDP+Generosity, data=happyT)
plot(mod2)
```

Both qqplots for the different transformations look pretty good, and while it looks like the transform has helped improve homoscedasticity, my large sample size means I can relax this assumption a bit as well.
Just to be sure, though, I can check homoscedasiticity using the bp test.

```{r echo=TRUE}
bptest(mod1)
```
```{r echo=TRUE}
bptest(mod2)
```

Model 1 still does not pass the bp-test with a p-value of 0.01. Model 2 does pass the bp-test.

I can also check for multi-collinearity in my values.

```{r echo=TRUE}
vif(mod1)
vif(mod2)
```

With Values around 1, which are quite low, I don't need to worry about multi-collinearity in either model (they are actually thes ame, sinc ethey are running the same independent variables). 

In addition to looking at the correlation values, we can also use the DW test to check if our variables are highly correlated.
```{r}
dwtest(mod1)
dwtest(mod2)
```
Because we had relatively large p-values, our data is not autocorrelated in either model. 

I ran a shaprio test to see if one transformation helped me achieve normality.
```{r echo=TRUE}
shapiro.test(happyT$sqrtcor)
shapiro.test(happyT$logcorupt)

```

We can see that the square root transform did not help the data pass the normality test, while the log transform does. Because the log transform also meets all the other assumptions discussed above, I will use the log transformed corruption variable as the independent variable in my model.

```{r}
summary(mod2)
```
 According to this model, Freedom is is significantly assosciated with the square root of corruption values. GDP is not significantly assosciated with the square root of corruption values, and Generosity is not significantly assosciated with the square root of the corruption values.
 
The model fit isn't great with an adjusted R^2^ = 0.20. Our model is only able to explain 20% of the variance in the data. 


###Question 5

Choose one of the continuous independent variables that was significant in the model and interact it with region to predict corruption. Does the influence of the continuous variable on corruptoin vary by region? If yes, how do you interpret the interaction?

```{r}
mod.int=lm(logcorupt~Freedom*Region, data=happyT)
summary(mod.int)
```

The influence of freedom on corruption does significantly vary as it interacts with region.  It effects the AfricaMideast (which is set as the intercept at this point with a ver small p-value, as well as the Europe with a very small p-value. 


```{r}
interplot(m=mod.int, var1="Freedom", var2="Region") + labs(x="Region", y="Estimated coefficient for Freedom", title ="Estimated Coefficient of Freedom on Region")
```

This interaction further shows that  there is a significant difference on whether freedom interacts with region if you are in AfricaMideast or Europe.


###Question 6
Which factors are significantly assosciated with whether a breast cancer tumor is malignant or not? Choose three continuous independent variables to include in the model.


```{r}
pairs(cancer)
```

Looking at the chart above, it looks like Radius, Texture, and Smoothness are good candidate variables. 
 
We can check to see if they are correlated. 
```{r}
cor(cancer[, c("radius_mean","texture_mean","smoothness_mean")])
```

With values all falling below 0.5, we should not run into issues of highly correlated variables. 


```{r}
mal.mod=glm(malignant~radius_mean+texture_mean+smoothness_mean, family="binomial", data=cancer)
summary(mal.mod)
```

The model shows us that the probability for a tumor being malignant given the mean radius, mean texture, and mean smoothness of the tumor. From the output, the as the mean radius, the mean texture, and the mean smoothness all go up, the tumor is more likely to be malignant. 

###Bonus

Which independent variables are most important in explaining whether a breast cancer tumor is malignant or not?

```{r}
summary(mal.mod)
varImp(mal.mod, scale=FALSE)
```

It looks like Radius, then Texture, then Smoothness is the order of importance.