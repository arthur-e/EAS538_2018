---
title: "danjtan_TakeHome_Final_Exam"
author: Daniel Tanner (umich ID = danjtan)
umich ID: danjtan
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
   toc: True
   toc_depth: 3
   toc_float: True
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, fig.align='center', message = FALSE, warning = FALSE)
```

```{r, include=FALSE}
library(car)
library(ggplot2)
library(lmtest)
```

```{r, include=FALSE}
##Define panel functions for correlation analysis
panel.lm <- function (x, y,  pch = par("pch"), 
    col.lm = "red",  ...) 
{   ymin <- min(y)
    ymax <- max(y)
    xmin <- min(x)
    xmax <- max(x)
    ylim <- c(min(ymin,xmin),max(ymax,xmax))
    xlim <- ylim
    points(x, y, pch = pch,ylim = ylim, xlim= xlim,...)
    ok <- is.finite(x) & is.finite(y)
    if (any(ok)) 
        abline(lm(y[ok]~ x[ok]), 
            col = col.lm, ...)
}

panel.cor <- function(x, y, digits=2, prefix="", cex.cor){
  usr = par("usr"); on.exit(par(usr)) 
  par(usr = c(0, 1, 0, 1)) 
  r = cor(x, y)
  txt = format(c(r, 0.123456789), digits=digits)[1] 
  txt = paste(prefix, txt, sep="") 
  if(missing(cex.cor)) cex = 0.8/strwidth(txt) 
  
  test = cor.test(x,y) 
  # borrowed from printCoefmat
  Signif = symnum(test$p.value, corr = FALSE, na = FALSE, 
                  cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                  symbols = c("***", "**", "*", ".", " ")) 
  
  text(0.5, 0.5, txt, cex=3) 
  text(0.8, 0.8, Signif, cex=3, col=2) 
}
```

This exam is open book and open internet but you are NOT allowed to work with anyone else or ask anyone other than Meha or Oscar any questions about the exam. It is due at noon on Sunday, April 23. 

Please answer the following questions by analyzing the associated datasets. For all tests, please:
-	check whether the data meet the requirements/assumptions of the test you plan to run
-	complete any transforms needed to make the data meet the required assumptions
-	run the test
-	interpret the results
-	check model fit in the case of linear regressions and/or glms
-	if you have the option between running a linear model with a transformed y variable or a glm, choose the linear model with a transformed y variable. only run a glm when you have to.

Provide all answers in R or R markdown (similar to the take home quiz 4). Use the following scripts to load the datasets. The dataset to be used for each question is provided in bold at the end of the question.

Load Data:
```{r}
flying = read.table(file="https://raw.githubusercontent.com/OscarFHC/NRE538_2017Fall/master/Final/flying.csv",header=TRUE, sep=",")

college = read.table(file="https://raw.githubusercontent.com/OscarFHC/NRE538_2017Fall/master/Final/college.csv",header=TRUE, sep=",")

happy = read.table(file="https://raw.githubusercontent.com/OscarFHC/NRE538_2017Fall/master/Final/happy.csv",header=TRUE, sep=",")

cancer = read.table(file="https://raw.githubusercontent.com/OscarFHC/NRE538_2017Fall/master/Final/cancer.csv",header=TRUE, sep=",")

```

#Problem 1
_Is there a significant association between gender (`gender`) and whether people think it’s rude to bring an unruly child on the plane (`unruly_child`)?_ **flying**

Both the independent and dependent variables are categorical, so I will perform a **Chi-square test for independence**.

##Assumptions

###Contingency Table
```{r}
flying.tbl <- table(flying$gender, flying$unruly_child)
flying.tbl
```
There are no structural zeros in the contingency table for this scenario and no very low values (<5) in the contingency table for our actual data, so our assumptions are met.

##Test and Interpretation

```{r}
chisq.test(flying.tbl)
```
The p-value is less than 0.05, so we reject the null hypothesis that `gender` and `unruly_child` are independent. Thus gender is significantly associated with whether or not a passenger thinks bringing an unruly child onto an airplane is rude.


#Problem 2
_Is there a significant difference in tuition (`tuition`) by type of institution (`type`)?_ **college**

```{r}
levels(college$type)
```

There are only two different `type`s of colleges in our data so I know I want to run a **two sampled unpaired t-test**.

##Assumptions

###Independent & Random Observations
I assume that these schools represent a random sample of schools, since I can't do anything otherwise.


###Normality
```{r}
shapiro.test(college$tuition)
```
The sample does not have normally distributed `tuition` values, but there are more than 30 observations so the t-test is still appropriate.

###Equal Variance

```{r}
priv.tuition <- subset(college, type == "Private nonprofit")$tuition
pub.tuition <- subset(college, type == "Public")$tuition
```
```{r}
var.test(priv.tuition, pub.tuition)
```

Our _F-test_ concluded that the variances are unequal, so I will use a **Welch's t-test for unequal variance**.

##Test and Interpretation

```{r}
t.test(priv.tuition, pub.tuition, var.equal = FALSE)
```
With a very small p-value we can reject the null hypothesis that the there is no difference between the sample means. Thus we conclude that the sample means are significantly different. Since the mean of the sample of private institutions is greater than the mean of public institutions, we conclude that Private Nonprofit type institutions are significantly more expensive than public institutions.


#Problem 3
_Is there a significant difference in happiness (`Hscore`) by region (`Region`)?_ **happy**

```{r}
levels(happy$Region)
```
Our dependent vairable (`Hscore`) is continuous while our independent variable (`Region`) is categorical with more than two levels. This means we will try to use an **ANOVA** test.

##Assumptions

###Independent Observations

We again, take this for granted. Hopefully the scientists collecting these data had good methodology!

###Normality
```{r}
shapiro.test(happy$Hscore)
```
```{r}
hist(happy$Hscore, breaks = 20, col = "orange")
```


The histogram doesn't look _terrible_, but we reject the null hypothesis that `Hscore` is normally distributed.


```{r}
shapiro.test(log(happy$Hscore))
shapiro.test(sqrt(happy$Hscore))
```
There is not an obvious transform to achieve normality, so I will procede with the **Kruskal-Wallis** test which allows me to relax the normality assumption.

###Equal Variance
```{r}
leveneTest(happy$Hscore, happy$Region)
```
  
With a p-value of 0.52 we fail to reject the null hypothesis that there is a significant difference in `Hscore` variance between groups. Thus our data meet the variance assumption of the ANOVA test.
  
##Test and Interpretation

```{r}
kruskal.test(Hscore~Region, data = happy)
```

```{r}
library(dunn.test)
dunn.test(happy$Hscore, happy$Region, kw = TRUE, method = "hs")
```
We see that there are significant differences in happiness between regions, which align with the understanding we get from a standard box and whisker plot:

```{r}
ggplot(happy, aes(x=as.factor(Region), y=Hscore, fill=as.factor(Region))) + geom_boxplot() + labs(y="Happiness Score",x="Region", title = "Happiness Score by Region") + guides(fill=FALSE)

```
Africa has significantly different happiness scores from each other region. The smaller differences between the other three regions are sometimes significant (Americas vs. Asia) and sometimes not.

#Problem 4
_What factors are significantly associated with a country’s corruption levels (`Corruption`)? Choose three continuous independent variables to include in your model._ **happy**

```{r, include = FALSE}
pairs(happy[, c(9, 10,4,5,6,7,8)], lower.panel = panel.lm, upper.panel = panel.cor)
```

I'm going to look at the relationship between `Corruption` and `Life`, `Freedom`, and `Generosity` by using a **multivariate linear model**. 

```{r}
pairs(happy[, c("Corruption", "Life", "Freedom", "Generosity")], lower.panel = panel.lm, upper.panel = panel.cor)
```

The largest pearson correlation coefficient between the independent variables is 0.36, which is below the 0.5 threshold discussed in class for when to be concerned about multi-colinearity.

```{r}
shapiro.test(happy$Corruption)
```
```{r}
hist(happy$Corruption)
```
We can see that `Corruption` scores are not normally distributed; they have a strong right skew. I will try a log transform:

```{r}
shapiro.test(log(happy$Corruption))
```
```{r}
hist(log(happy$Corruption), breaks = 20)
```
Much better.

##Assumptions

```{r}
happy$log.corruption <- log(happy$Corruption)
mod1 <- lm(log.corruption ~ Life + Freedom + Generosity, data = happy)
```

###Linear Relationship

```{r}

pairs(happy[, c("log.corruption", "Life", "Freedom", "Generosity")], lower.panel = panel.lm, upper.panel = panel.cor)
```
The above plots between `log.corruption` and each of the independent variables suggest plausible linear relationships, and don't present any obvious non-linear relationships.

###Homoscedastisity

```{r}
plot(mod1$residuals)
```

```{r}
bptest(mod1)
```
With a p-value of 0.12 we fail to reject the null hypothesis that the residuals are homoscedastic. This we meet our assumption.

###Independence of Errors

```{r}
dwtest(mod1)
```
With a p-value of 0.178 we fail to reject the null hypothesis that the residuals are not autocorrelated.

###Normality of Residuals

```{r}
shapiro.test(residuals(mod1))
```
```{r}
plot(mod1)
```
From our shapiro-wilk test we reject the null hypothesis that the residuals are normally distributed. The qq-plot doesn't look great, but it could be worse, so I'm going to use this model anyway.


##Test and Interpretation

```{r}
summary(mod1)
```

According to my model, `Life` and `Generosity` are not significantly associated with corruption. However, `Freedom` is significantly associated with corruption (p = 6.31e-07). For every one point increase of `Freedom` score, our model predicts that the log of corruption will increase by 2.26 points. It is surprising to find a positive correlation between these measures. Our model fit, however, is not very good. According to our adjusted R-squared statistic, our model only explains 19.2% of the variance in `log.corruption`. This is not a great fit.


#Problem 5
_Choose one of the continuous independent variables that was significant in the model for Question 4 and interact it with region (`Region`) to predict corruption (`Corruption`). This model should only include one continuous independent variable and its interaction with region. Does the influence of your continuous variable on corruption vary by region?_ **happy**

```{r}
mod2 <- lm(log.corruption ~ Freedom + Region + Freedom * Region, data = happy)
```

##Assumptions

###Linear Relationship
Same as above for `Freedom`.

###Homoscedasticiy

```{r}
bptest(mod2)
```
With a p-value of 0.25 we fail to reject the null hypothesis that the residuals are homoscedastic. This we meet our assumption.

###Independence of Errors
```{r}
dwtest(mod2)
```
With a p-value of 0.7187 we fail to reject the null hypothesis that the residuals are not autocorrelated.

###Normality of Residuals
```{r}
plot(mod2)
```
The qq-plot again runs awry a bit on the extremes, but it's close enough that I'm going to use it. In reality I have failed to meet the normality assumption.

##Interpretation

```{r}
summary(mod2)
```
Again we see a significant effect of `Freedom` on the log of corruption. For each increase of `Freedom` score in the "AfricaMideast"region, we the model expects an increase in the log of `Corruption` by 1.59 points. The regions "AmericasCarribean" and "AsiaAustralia" do not have significant differences from this relationship. However, in the region "Europe", the model expects an overall reduction in  `log.corruption` score by 1.3755 points. There is also a significant interaction term, meaning that in Europe, each increase in `Freedom` score is expected to correspond with an subsequent increase of 1.5929 + 2.9826 = 4.5755 in the log of the corruption score.

This model fits better than my last one. 30.53% of the variance in `log.corruption` is explained by the model. It still does not explain very much.


#Problem 6
_Which factors are significantly associated with whether a breast cancer tumor is malignant or not? Choose three continuous independent variables to include in your model._ **cancer**

I'm going to check whether the tumor's size (`radius_mean`), texture (`texture_mean`), or smoothness (`smoothness_mean`) affect whether or not it is `malignant`.

```{r}
pairs(cancer[, c("malignant", "radius_mean", "texture_mean", "smoothness_mean")], lower.panel = panel.lm, 
    upper.panel = panel.cor)
```

My independent variables don't have any pairwise correlation of concerning levels, so I'm not worried about colinearity. My dependent variable is binomial, so I am going to use a **GLM with the binomial distribution**. 

```{r}
mod3 <- glm(malignant ~ radius_mean + texture_mean + smoothness_mean, data = cancer, family = binomial)
```
I let the model use the default **logit** link funcion.

##Assumptions

###Independence
Again, these are not my data. I assume whoever collected them made sure they were independent (for example, it would be unwise to include multiple tumors from the same person in these data).

##Interpretation

```{r}
summary(mod3)
```
All three dependent variables are significant, with very small p-values. An increase of mean radius by one unit is predicted to increase the log odds of the tumor being malignant by 1.39699. An increase in `texture_mean` by one unit is predicted to increase the log odds of the tumor being malignant by 0.38056. An increase in `smoothness_mean` by one unit is predicted to increase the log odds of the tumor being malignant by 144.67423. We are unable to compare which of these factors has the most impact on malignancy, however, because they are each measured on different scales.


#Problem 7 - Extra Credit
_Which independent variables are the most important in explaining whether a breast cancer tumor is malignant or not? Use the same 3 continuous independent variables you chose for question 6._ **cancer**

```{r}
mod4 <- glm(malignant ~ scale(radius_mean) + scale(texture_mean) + scale(smoothness_mean), data = cancer, family = binomial)
```
```{r}
summary(mod4)
```

By scaling and centering the variables to standard deviation, we can see which is expected to have the largest effect on malignancy. It looks like size is most correlated with malignancy, followed by smoothness then texture.
