---
title: "EAS 538 Linear regression"
author: "Written by Oscar Feng-Hsun Chang and Meha Jain, and modified by Arthur Endsley"
date: "Week 7"
output: 
  html_document:
    code_folding: show
    highlight: textmate
    keep_md: yes
    number_sections: true
    theme: flatly
    toc: yes
    toc_float:
      collapsed: false
      smooth_scroll: true
      toc_depth: 4
---

\newcommand\expect[1]{\mathbb{E}\left[{#1}\right]}
\newcommand\var[1]{\mathrm{Var}\left[{#1}\right]}

```{r, set global theme, include=FALSE}
knitr::opts_chunk$set(fig.width=10, fig.height=10)
```

```{r, library loading, echo=FALSE, message=FALSE, warning=FALSE}
library(rmarkdown)
library(knitr) # Run R Code Chunks
library(ggplot2) # For plotting
library(RCurl)
library(dplyr)
library(tidyr)
library(scales)
```

Let's use [air quality data in New York](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airquality.html) to demonstrate today's topics. To learn more about the dataset and what variables are included in it, please use the `?` command.

```{r, airquality data,results='hidden',eval=FALSE}
data("airquality")
head(airquality)
?airquality
```



# Correlation
The correlation coefficient describes how two variables vary together in a linear fashion. The correlation coefficient is agnostic to which variable you define as x and which variable you define as y. In other words, you will get the same correlation coefficient if you define `Ozone` as `x` and `Wind` as `y` as if you define `Ozone` as `y` and `Wind` as `x`. Let's check this out:

```{r, cor1,results='hidden',eval=FALSE}
cor(airquality$Ozone,airquality$Wind,use='complete.obs')
cor(airquality$Wind,airquality$Ozone,use='complete.obs')
```

`complete.obs` means that we just use the rows where there is both a value for `Ozone` and `Wind` (sometimes there are missing values in either your x or y variable).


## Correlation plots

Let's now look at the correlation across all of our different continuous variables in the dataset. We can first use `pairs()` to create a series of scatterplots for all pairings of the variables included so we can see visually how each variable varies with another.

```{r, cor2,results='hidden',eval=FALSE}
pairs(airquality[, c("Ozone", "Solar.R", "Wind", "Temp")])
```

## Pearson correlation coefficient
We can then calculate the correlation coefficient all at once by using `cor()`. 

```{r, correlation coefficient2,results='hidden',eval=FALSE}
cor(airquality[, c("Ozone", "Solar.R", "Wind", "Temp")], use="complete.obs")
```
---------------------------------------------------------------------------------------------------------------------------------
__Exercise 1__  
1. What is the correlation between `Ozone` and `Wind`? Please describe the meaning of this correlation value in non technical terms (e.g., as Ozone increases, Wind...). Is this correlation strong in your opinion?  
2. What is the correlation between `Ozone` and `Solar.R`? Please describe the meaning of this correlation value in non technical terms. Is this correlation strong in your opinion?  
---------------------------------------------------------------------------------------------------------------------------------  

## Linear association

Correlation is only used to investigate the <span style="color:red">_linear association_</span> between two variables. It could be misleading if you don't plot the two variables to see if the two variables are really correlated with each other.  

Let's do a small exercise. Run the following chunk of code and you should have 3 vectors (`var1`, `var2`, and `var3`) each containing 1200 numbers. Don't worry if you don't understand the code below.

```{r, generate Ys}
set.seed(15165)
f = function(x){
 y=-(x+3)^2 + runif(length(x), min=-2, max=2)
}
f1 = function(x){
  y=runif(length(x), min=min(x), max=max(x))
}

x=seq(from=-6, to=0, by =0.005)
var1=f(x)
var2=f1(x)
var3=x
```

Calculate the correlation coefficient between `var1` and `var2` as well as `var1` and `var3` by using `cor(x,y)`. 

```{r, cor coef, results='hidden',eval=FALSE}
cor(var1,var2)
cor(var1,var3)
```

Now plot `var1` vs `var2` and `var1` vs `var3` to see the relationship between them.

```{r, plot ys vs x, eval=FALSE,results='hidden'}
plot(var2~var1)
abline(lm(var2~var1))
plot(var3~var1)
abline(lm(var3~var1))
```

Although the correlations are nearly 0, the main problem is we shouldn't have been using a correlation coefficient to see how these variables are related in the first place since they are related in a _nonlinear_ way. 

---------------------------------------------------------------------------------------------------------------------------------
__Exercise 2__  
1. Try to plot and calculate the correlation coefficients between the waiting time between eruptions (i.e. `waiting`) and the duration of the eruption (i.e. `eruptions`) for the Old Faithful geyser in Yellowstone National Park. The dataset is built in R and is named [faithful](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/faithful.html). 
---------------------------------------------------------------------------------------------------------------------------------

```{r, faithful cor, eval=FALSE,results='hidden'}
head(faithful)
```

# Regression

In a simple, univariate linear regression, the goal is to predict y (your dependent variable) with any given x value (your independent variable). The regression line is the best fit line that minimizes the error between your observed y variables (i.e. the y variables actually in your sample dataset) and your predicted y variables (i.e. the y variable that is predicted by your linear regression line at any given point of x). These predicted y values should be as close to each observed y value as possible. The overall error of your model is described as the [_Residual Sum of Square_ (RSS)](https://en.wikipedia.org/wiki/Residual_sum_of_squares) also known as the sum of square errors. 

## Linear model

Let's use the airquality data to perform a linear regression in __R__ by using `lm()`. Unlike with the correlation, which variable you define as x and which variable you define as y is important.  

```{r, lm,results='hidden',eval=FALSE}
mod = lm(Temp~Wind, data=airquality)
summary(mod)
```
Interpret your model results. What is the relationship between wind and temperature? Is this relationship significant? 

We can plot temperature against wind and the estimated regression line (red line) to visualize our data and the linear regression.  

```{r, regression plot,results='hidden',eval=FALSE}
plot(Temp~Wind, data=airquality)
abline(lm(Temp~Wind, data=airquality), col="red")
```
Qualitatively, how well do you think your linear model fits the data? 

---------------------------------------------------------------------------------------------------------------------------------

__Exercise 3__  
1. Build a second linear model using `Temp` as your dependent variable and `Ozone` as your independent variable. Interpret your linear model results. Plot a scatterplot of your variables with an estimated regression line.   
2. Build a third linear model using the `faithful` dataset to examine if you see longer erruption times if you wait longer. Interpret your linear model results. Plot a scatterplot of your variables with an estimated regression line.   

---------------------------------------------------------------------------------------------------------------------------------

## Errors
Let's understand what the errors are a bit more in depth. You can get the errors, also known as residuals in __R__ using the following code: 

```{r, res se,results='hidden',eval=FALSE}
mod = lm(Temp~Wind,data=airquality)
res = residuals(mod)
```
What the best fit line in a linear regression is minimizing is the sum of the square of your `res` term above (sum of squared errors!). Remember why we square the data before taking the sum (think back to why we did this when calculating variance)? 

```{r, min,results='hidden',eval=FALSE}
sum(residuals(mod)^2)
```

Let's see if we can calculate this manually. This will really ensure we understand what residuals are! We will need both the observed y (in this case the `Temp` data in our sample) and the predicted y (the predicted temperature we get from our linear regression)

```{r, resid,results='hidden',eval=FALSE}
y = airquality$Temp
x <- data.frame(Wind = airquality$Wind) # our x variable has to be in the format of a dataframe in the predict function below
predy = predict.lm(mod,x)
res2 = y-predy
head(res)
head(res2)
```
They match!!!!! Congratulations. You deserve a quick break (https://www.youtube.com/watch?v=eeuhdMNRsgE)!  

## Goodness of fit
After fitting a linear model to the data, we can ask "how well does the linear model actually fit my data?".   

### $R^2$
The first thing we can do is to see how much of the variance is being explained by the indpendent variables we put in the model. This is what the R2 value is telling you. 

```{r, R2,results='hidden',eval=FALSE}
mod = lm(Temp~Wind,data=airquality)
summary(mod)
r2 = summary(mod)$r.squared
```
Meha showed in lecture what the R2 really is and how we can calculate it. The formula is:  

R2 = SSY'/SSY   

As a reminder, it's the amount of variation in the observed y variable (i.e. in this case, `Temp` in your sample data) that can be explained by your linear model.   

Remember, this follows from the more general formula for partitioning the sum of squared errors:  

SSY = SSY' + SSE    

Okay, let's see how calculating the R2 actually works. First, let's calculate the amount of variation we have in our observed y variable, and let's name this `SSY`:

```{r,yvar var}
y = airquality$Temp
meany = mean(y)
diff = y-meany
SSY = sum(diff^2)
```
Think back to when we calculated variance a few weeks back (the dog example in lecture). This is the EXACT SAME THING, we are just calculating the variance in our observed y variable.   

Okay, now let's calculate the amount of variation that is being explain by your linear model, termed `SSYp` which stands for SSY' in the lecture slides and in the equation above.

```{r,lvar var,results='hidden',eval=FALSE}
x <- data.frame(Wind = airquality$Wind)
predy = predict.lm(mod,x)
meany = mean(y)
diff = predy-meany
SSYp = sum(diff^2)
```
You can think of this measure as the variance of your predicted y variable when using the mean of your observed data as the reference value.   

Now, to calculate the R2, we just take the ratio of SSYp over SSY. In other words, this is the percent of the variation in your observed y variable that is explained by the variation of your predicted variable. 

```{r2 calc, results='hidden',eval='FALSE'}
R2 = SSYp/SSY
summary(mod)
```
They match!!! You guys are pros at this! As another treat, check out: https://www.youtube.com/watch?v=0Bmhjf0rKe8  

---------------------------------------------------------------------------------------------------------------------------------
__Exercise 4__  
1. Remember that SSY is comprised of both SSY' and SSE. Please calculate SSE. I know this may be hard, but you can do it! What is SSE measuring in non technical terms? 
---------------------------------------------------------------------------------------------------------------------------------


### Model checking

As you all know, there are several assumptions that must be met with a linear regression. Let's see how we can check these assumptions in R. 

#### Constant variance of residuals
Remember homoscedasticity and heteroscedasticity? We should check to see if our errors are homoscedastic! We can do this in several ways. One of the most common ways is to just plot your x and y variable and visually inspect whether the variance of y changes based on where you are in the x axis (https://en.wikipedia.org/wiki/Homoscedasticity#/media/File:Homoscedasticity.png). Let's try this for the `Temp` and `Wind` regression.

```{r, plot data,results='hidden',eval=FALSE}
plot(Temp~Wind,data=airquality)
```
What do you think? It's sometimes hard to tell just by visually inspecting the scatterplot of your x and y variable. We can instead plot the residuals along the x variable and see if that helps with our qualitative interpretation.

```{r, plot resid,results='hidden',eval=FALSE}
plot(res~Wind,data=airquality)
```
This is a bit clearer to interpret. It seems like maybe there is a bit more variance in the middle values of wind (around 10), but I'm still not entirely sure. Luckily we're using R and since R is amazing it has some built in functions that we can use to statistically test for homoscedasticity. These tests are in the packages `car` and `lmtest`. Please install these packages first if you don't have them already. 

```{r, bptest, message=FALSE,results='hidden',eval=FALSE}
#install.packages('lmtest')
#install.packages('car')
library(lmtest)
library(car)
bptest(mod)
ncvTest(mod)
```
Both of these results show that our p value is > 0.05, and therefore it shows that our residuals are homoscedastic. If you want to read more about these tests please use `?` and you can check out this website (https://www.r-bloggers.com/how-to-detect-heteroscedasticity-and-rectify-it/). In a few weeks we'll talk about what you can do if your residuals are not homoscedastic (and instead are heteroscedastic), but the article touches on some ideas in case you want a sneak peek. 

#### Normal distributed residuals
Another assumption of the regression is that your errors are normally distributed. Remember the QQ plot and the shapiro-wilk test we used before to check for normality? We can use them again to check for normality of our residuals.

```{r, resid qqplot,results='hidden',eval=FALSE}
qqPlot(residuals(mod))
```
Like Meha said in lecture, as long as your residuals fall within the dotted red lines, your residuals are mostly normal and you don't have to worry. To learn more about what the dotted lines represent you can read the help function of `qqPlot` using `?`. Generally what they are a 95% confidence interval around an expected normal distribution.  

We can also use a test to statistically see whether our residuals are normal. This is the Shapiro-Wilk test!
```{r, shapiro.test, results='hidden',eval=FALSE}
shapiro.test(residuals(mod))
```
What is the p value? What does it suggest about whether your residuals are normal?

From the QQ plot and the shapiro-wilk test, we can say that the residuals are normally distributed. That's great! 

#### Independence of the residuals (autocorrelation)
Another thing to check with your residuals is whether they are autocorrelated. This means that there is some trend in your residuals. This is the example Meha gave in lecture of time series data and having data points that are correlated through time (the auto sales example). We will get into how you would test for this and deal with this later in the class, so let's not worry about it for now. 


> Always check!  
> 1. <span style="color:red"> residual homoscedasticity (possibly with the `bptest` or `ncvTest`) </span>  
> 2. <span style="color:red"> residual normality (possibly with the `shapiro.test`)</span>  
> 3. <span style="color:red"> residual independency (but don't worry about how to do this for now!) </span>  

---------------------------------------------------------------------------------------------------------------------------------

__Exercise 5__

1. Create a linear model using the `airquality` dataset with `Temp` as your dependent variable and `Ozone` as your independent variable. Check whether your model meets the assumptions of homoscedasticity and normality using the plots and tests we used above. 

---------------------------------------------------------------------------------------------------------------------------------