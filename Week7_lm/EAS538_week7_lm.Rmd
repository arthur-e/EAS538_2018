---
title: "EAS 538 Linear regression"
author: "Written by Oscar Feng-Hsun Chang and Meha Jain, and modified by Arthur Endsley"
date: "Week 7"
output: 
  html_document:
    code_folding: show
    highlight: textmate
    keep_md: no
    number_sections: true
    theme: flatly
    toc: yes
    toc_float:
      collapsed: false
      smooth_scroll: true
      toc_depth: 4
---

\newcommand\expect[1]{\mathbb{E}\left[{#1}\right]}
\newcommand\var[1]{\mathrm{Var}\left[{#1}\right]}

```{r, set global theme, include=FALSE}
knitr::opts_chunk$set(fig.width=10, fig.height=10)
```

```{r, library loading, echo=FALSE, message=FALSE, warning=FALSE}
library(rmarkdown)
library(knitr) # Run R Code Chunks
library(ggplot2) # For plotting
library(RCurl)
library(dplyr)
library(tidyr)
library(scales)
```

Let's use [air quality data in New York](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airquality.html) to demonstrate today's topics. To learn more about the dataset and what variables are included in it, please use the `?` command.

```{r, airquality data,results='hidden',eval=FALSE}
data("airquality")
head(airquality)
?airquality
```

# Correlation

The correlation coefficient describes how two variables vary together in a linear fashion. The correlation coefficient is the same regardless of which variable you define as `x` and which variable you define as `y`. Similarly, in __R__, you will get the same result regardless of the order of your arguments to the `cor()` function. In other words, you will get the same correlation coefficient if you define `Ozone` as `x` and `Wind` as `y` as if you define `Ozone` as `y` and `Wind` as `x`. Let's check this out:

```{r, cor1,results='hidden',eval=FALSE}
cor(airquality$Ozone, airquality$Wind, use = 'complete.obs')
cor(airquality$Wind, airquality$Ozone, use = 'complete.obs')
```

`complete.obs` means that we just use the rows where there is both a value for `Ozone` and `Wind` (sometimes there are missing values in either your x or y variable).

## Correlation plots

Let's now look at the correlation across all of our different continuous variables in the dataset. We can first use `pairs()` to create a series of scatterplots for all pairings of the variables included so we can see visually how each variable varies with another.

```{r, cor2,results='hidden',eval=FALSE}
pairs(airquality[, c("Ozone", "Solar.R", "Wind", "Temp")])
```

## Pearson correlation coefficient

We can then calculate the correlation coefficient all at once by using `cor()`. 

```{r, correlation coefficient2, results='hidden', eval=FALSE}
cor(airquality[, c("Ozone", "Solar.R", "Wind", "Temp")], use = "complete.obs")
```

---------------------------------------------------------------------------------------------------------------------------------

__Exercise 1__ (No need to show any R code) 

1. What is the correlation between `Ozone` and `Wind`? Please describe the meaning of this correlation value in *non-technical* terms (e.g., as Ozone increases, Wind...). In your opinion, is this a strong correlation?
2. What is the correlation between `Ozone` and `Solar.R`? Please describe the meaning of this correlation value in
*non-technical* terms. In your opinion, is this a strong correlation?  

---------------------------------------------------------------------------------------------------------------------------------  

## Linear association

Correlation is only used to investigate the <span style="color:red">__linear association__</span> between two variables. It could be misleading if you don't plot the two variables to see if the two variables are really correlated with each other.  

Let's do a small exercise. Run the following chunk of code and you should have 3 vectors (`var1`, `var2`, and `var3`) each containing 1200 numbers. Don't worry if you don't understand the code below.

```{r, generate Ys}
set.seed(15165)

f <- function(x) {
 y <- -(x+3)^2 + runif(length(x), min = -2, max = 2)
}

f1 <- function(x) {
  y <- runif(length(x), min = min(x), max = max(x))
}

x <- seq(from = -6, to = 0, by = 0.005)
var1 <- f(x)
var2 <- f1(x)
var3 <- x
```

Calculate the correlation coefficient between `var1` and `var2` as well as `var1` and `var3` by using `cor(x,y)`. 

```{r, cor coef, results='hidden',eval=FALSE}
cor(var1, var2)
cor(var1, var3)
```

Now plot `var1` vs `var2` and `var1` vs `var3` to see the relationship between them.

```{r, plot ys vs x, eval=FALSE,results='hidden'}
plot(var2 ~ var1)
plot(var3 ~ var1)
```

Although the correlations are nearly 0, the main problem is we shouldn't have been using a correlation coefficient to see how these variables are related in the first place since they are related in a _nonlinear_ way. 

There's a chapter in Kieran Healy's book on data visualization that [shows an even wider array of very different datasets that all have the same correlation coefficient](http://socviz.co/dataviz-pdfl_files/figure-html/ch-01-correlations-1.png) (and trendline). 
[Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) is another example; it consists of four graphs that not only have a similar correlation coefficient but a similar mean and standard deviation. When you look at the four graphs, however, there are obvious and significant differences between the underlying datasets.

These examples point to the importance of **exploratory data visualization;** always examine your data visually before analyzing it.

---------------------------------------------------------------------------------------------------------------------------------

__Exercise 2__ (R code and outputs required)

1. Plot the waiting time between eruptions (`waiting`) against the duration of the eruption (`eruptions`) for the Old Faithful geyser in Yellowstone National Park. The dataset is built in R and is named [faithful](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/faithful.html). 
2. Calculate the correlation coefficient between the waiting time (`waiting`) and duration of eruption (`eruptions`).

```{r, faithful cor, eval=FALSE,results='hidden'}
head(faithful)
help(faithful)
```

---------------------------------------------------------------------------------------------------------------------------------

# Regression

In a simple, *univariate* linear regression, the goal is to predict $y$ (your dependent variable) with any given $x$ value (your independent variable). The regression line is the best fit line that minimizes the error between your observed $y$ variables (i.e. the $y$ variables actually in your sample dataset) and your predicted y variables (i.e. the $y$ variable that is predicted by your linear regression line at any given point of $x$). These predicted $y$ values should be as close to each observed $y$ value as possible. The overall error of your model is described as the [_Residual Sum of Square_ (RSS)](https://en.wikipedia.org/wiki/Residual_sum_of_squares) also known as the sum of squared errors. 

## Linear model

Let's use the `airquality` data to perform a linear regression in __R__ by using the `lm()` function. Unlike with calculating the correlation, it makes a big difference which variable you define as $x$ (the independent or explanatory variable) and which variable you define as $y$ (the dependent or response variable).  

```{r, lm, results='hidden', eval=FALSE}
mod <- lm(Temp ~ Wind, data = airquality)
summary(mod)
```

What is the relationship between wind speed and temperature? Is this relationship significant? 

We can plot temperature against wind speed and the estimated regression line (red line) to visualize our data and the linear regression.  

```{r, regression plot, results='hidden', eval=FALSE}
plot(Temp ~ Wind, data = airquality)
abline(lm(Temp ~ Wind, data = airquality), col = "red")
```

Qualitatively, how well do you think your linear model fits the data? 

---------------------------------------------------------------------------------------------------------------------------------

__Exercise 3__ (R code and outputs required)

1. Build a second linear model using `Temp` as your dependent variable and `Ozone` as your independent variable. Interpret your linear model results. Plot a scatterplot of your variables with an estimated regression line.   
2. Build a third linear model using the `faithful` dataset to examine if you see longer erruption times if you wait longer. Interpret your linear model results. Plot a scatterplot of your variables with an estimated regression line.   

---------------------------------------------------------------------------------------------------------------------------------

## Errors

Let's understand what the errors are a bit more in depth. You can get the errors, also known as **residuals** in __R__, using the following code: 

```{r, res se, results='hidden', eval=FALSE}
mod <- lm(Temp ~ Wind, data = airquality)
res <- residuals(mod)
```

What the best fit line in a linear regression is minimizing is the sum of the square of your `res` term above (sum of squared errors!). Remember why we square the data before taking the sum (think back to why we did this when calculating variance)? 

```{r, min, results='hidden', eval=FALSE}
sum(residuals(mod)^2)
```

Let's see if we can calculate this manually. This will really ensure we understand what residuals are! We will need both the observed $y$ (in this case the `Temp` data in our sample) and the predicted $y$ (the predicted temperature we get from our linear regression)

```{r, resid,results='hidden',eval=FALSE}
y <- airquality$Temp

# Our x variable has to be in the format of a data.frame when we use the predict() function below
x <- data.frame(Wind = airquality$Wind) 

predy <- predict(mod, x)
res2 <- y - predy
head(res)
head(res2)
```

They match!!! Congratulations. You deserve a quick break (https://www.youtube.com/watch?v=eeuhdMNRsgE)!  

## Goodness of fit

After fitting a linear model to the data, we can ask "how well does the linear model actually fit my data?".   

### $R^2$

The first thing we can do is to see how much of the variance is being explained by the independent variables we put in the model. This is what the $R^2$ value is telling you. 

```{r, R2,results='hidden',eval=FALSE}
mod <- lm(Temp ~ Wind, data = airquality)
summary(mod)
r2 <- summary(mod)$r.squared
```
Meha showed in lecture what the $R^2$ really is and how we can calculate it. The formula is:  

$R^2$ = $\frac{SSY'}{SSY}$   

As a reminder, it's the amount of variation in the observed $y$ variable (i.e. in this case, `Temp` in your sample data) that can be explained by your linear model.   

Remember, this follows from the more general formula for partitioning the sum of squared errors:  

$SSY = SSY' + SSE$    

Okay, let's see how calculating the $R^2$ actually works. First, let's calculate the amount of variation we have in our observed $y$ variable, and let's name this `SSY`:

```{r, yvar var}
y <- airquality$Temp
meany <- mean(y)
diff <- y - meany
SSY <- sum(diff^2)
```

Think back to when we calculated variance a few weeks back (the dog example in lecture). This is the EXACT SAME THING, we are just calculating the variance in our observed $y$ variable.   

Okay, now let's calculate the amount of variation that is being explain by your linear model, termed `SSYp` which stands for $SSY'$ in the lecture slides and in the equation above.

```{r lvar var, results='hidden', eval=FALSE}
x <- data.frame(Wind = airquality$Wind)
predy <- predict.lm(mod,x)
meany <- mean(y)
diff <- predy - meany
SSYp <- sum(diff^2)
```

You can think of this measure as the variance of your predicted $y$ variable when using the mean of your observed data as the reference value.   

Now, to calculate the $R^2$, we just take the ratio of `SSYp` over `SSY`. In other words, this is the percent of the variation in your observed $y$ variable that is explained by the variation of your predicted variable. 

```{r calc, results='hidden', eval=FALSE}
R2 <- SSYp/SSY
summary(mod)
```

They match!!! You guys are pros at this! As another treat, check out: https://www.youtube.com/watch?v=0Bmhjf0rKe8  

---------------------------------------------------------------------------------------------------------------------------------

__Exercise 4__ (R code and outputs required)  

1. Remember that `SSY` is comprised of both `SSYp` ($SSY'$) and `SSE`. Please calculate `SSE`. I know this may be hard, but you can do it! Then, verify that `SSY - SSYp` is equal to `SSE`. What is `SSE` measuring in non-technical terms? 

---------------------------------------------------------------------------------------------------------------------------------

### Model checking

As you all know, there are several assumptions that must be met with a linear regression. Let's see how we can check these assumptions in R. 

#### Constant variance of residuals

Remember homoscedasticity and heteroscedasticity? We should check to see if our errors are homoscedastic! We can do this in several ways. One of the most common ways is to just plot your x and y variable and visually inspect whether the variance of $y$ changes based on where you are in the $x$ axis (https://en.wikipedia.org/wiki/Homoscedasticity#/media/File:Homoscedasticity.png). Let's try this for the `Temp` and `Wind` regression.

```{r, plot data,results='hidden',eval=FALSE}
plot(Temp ~ Wind, data = airquality)
```

What do you think? It's sometimes hard to tell just by visually inspecting the scatterplot of your x and y variable. We can instead plot the residuals along the x variable and see if that helps with our qualitative interpretation.

```{r, plot resid, results='hidden', eval=FALSE}
res <- residuals(mod)
plot(res ~ Wind, data = airquality)
```

This is a bit clearer to interpret. It seems like maybe there is a bit more variance in the middle values of wind (around 10), but I'm still not entirely sure. Luckily we're using R and since R is amazing it has some built in functions that we can use to statistically test for homoscedasticity. These tests are in the packages `car` and `lmtest`. Please install these packages first if you don't have them already. 

In both the **Breush-Pagan test** (`bptest`) and the non-constant error variance test (`ncvTest`), the null hypothesis, $H_0$, is that the variance is constant (a null hypothesis of homoskedasticity).

```{r, bptest, message=FALSE, results='hidden', eval=FALSE}
#install.packages('lmtest')
#install.packages('car')
library(lmtest)
library(car)
bptest(mod)
ncvTest(mod)
```

Both of these results show that our p-value is > 0.05, therefore we *fail to reject* the null hypothesis of homoskedasticity; i.e., it shows that our residuals are homoscedastic. If you want to read more about these tests please use `?` and you can check out this website (https://www.r-bloggers.com/how-to-detect-heteroscedasticity-and-rectify-it/). In a few weeks we'll talk about what you can do if your residuals are not homoscedastic (and instead are heteroscedastic), but the article touches on some ideas in case you want a sneak peek. 

#### Normal distributed residuals

Another assumption of the regression is that your errors are normally distributed. Remember the QQ-plot and the Shapiro-Wilk test we used before to check for normality? We can use them again to check for normality of our residuals.

```{r, resid qqplot,results='hidden',eval=FALSE}
qqPlot(residuals(mod))
```

Like Meha said in lecture, as long as your residuals fall within the dotted red lines, your residuals are mostly normal and you don't have to worry. To learn more about what the dotted lines represent you can read the help function of `qqPlot` using `?`. Generally, what they are are a 95% confidence interval around an expected normal distribution.  

We can also use a test to statistically see whether our residuals are normal. This is the Shapiro-Wilk test!

```{r, shapiro.test, results='hidden', eval=FALSE}
shapiro.test(residuals(mod))
```

What is the p-value? What does it suggest about whether your residuals are normal?

From the QQ-plot and the Shapiro-Wilk test, we can say that the residuals are normally distributed. That's great! 

#### Independence of the residuals (autocorrelation)

Another thing to check with your residuals is whether they are autocorrelated. This means that there is some trend in your residuals. This is the example Meha gave in lecture of time series data and having data points that are correlated through time (the auto sales example). We will get into how you would test for this and deal with this later in the class, so let's not worry about it for now. 

> **Always check!**
>
> 1. <span style="color:red"> Residual homoscedasticity? (possibly with the `bptest` or `ncvTest`) </span>  
> 2. <span style="color:red"> Residual normality? (possibly with the `shapiro.test`)</span>  
> 3. <span style="color:red"> Residual independence? (but don't worry about how to do this for now!) </span>  

---------------------------------------------------------------------------------------------------------------------------------

__Exercise 5__ (R code and outputs required)

1. Create a linear model using the `airquality` dataset with `Temp` as your dependent variable and `Ozone` as your independent variable. Check whether your model meets the assumptions of homoscedasticity and normality using the plots and tests we used above. 

---------------------------------------------------------------------------------------------------------------------------------